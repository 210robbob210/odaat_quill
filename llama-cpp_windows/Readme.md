# Instructions for Windows install llama-cpp

to be able to run llama-cpp on windows on your GPU you have to follow the instructions in the word document.

Its a few steps that might hurt, but I'm pretty sure you can handle it ;)

In case it will not work and you have a RTC 3xxx card you might try the llama_cpp folder with in here.
just do a normal pip install llama-cpp-python and once thts finished just exchange the folder which is 
located in site-packages of your python environment with the folder here.

This is not tested at all, But my guess is it would work. Let me know how it went if you did try it.